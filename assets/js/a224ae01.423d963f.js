"use strict";(self.webpackChunkmy_wiki=self.webpackChunkmy_wiki||[]).push([[3208],{4837:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>a,contentTitle:()=>l,default:()=>d,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var s=i(5893),t=i(1151);const r={},l="Large Language Model",o={id:"computers/ai/llm",title:"Large Language Model",description:"Resources",source:"@site/docs/computers/ai/llm.md",sourceDirName:"computers/ai",slug:"/computers/ai/llm",permalink:"/wiki/computers/ai/llm",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Artificial Intelligence",permalink:"/wiki/computers/ai/"},next:{title:"Productize AI",permalink:"/wiki/computers/ai/productize"}},a={},c=[{value:"Resources",id:"resources",level:2},{value:"Links",id:"links",level:2}];function h(n){const e={a:"a",h1:"h1",h2:"h2",li:"li",ul:"ul",...(0,t.a)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.h1,{id:"large-language-model",children:"Large Language Model"}),"\n",(0,s.jsx)(e.h2,{id:"resources",children:"Resources"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://magazine.sebastianraschka.com/p/understanding-large-language-models",children:"Understanding large language models"}),"\n(",(0,s.jsx)(e.a,{href:"https://news.ycombinator.com/item?id=35589756",children:"HN"}),")","\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Top 10-ish papers to understand the design, constraints and evolution of\nLLMs"}),"\n",(0,s.jsx)(e.li,{children:"Development of LLMs: Attention weighted encodings, transformer, BERT, GPT,\nBART"}),"\n",(0,s.jsx)(e.li,{children:"Improving the efficiency of LLMs: FlashAttention, Cramming, finetuning\nmethods, Chinchilla model, InstructGPT, and more on reinforcement learning\nwith human feedback (RLHF)"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://willthompson.name/what-we-know-about-llms-primer",children:"What we know about LLMs (Primer) | Will Thompson"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"A simple explainer of what is considered an LLM, what we knew about LLMs and\nwhat are the ongoing research"}),"\n",(0,s.jsx)(e.li,{children:"Includes a lot of links to other resources. A few concepts introduced\ninclude LLMs' capability to generalize knowledge, power law in LLMs'\nperformance, reinforcement learning via human feedback (RLHF), etc."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://bbycroft.net/llm",children:"LLM Visualization"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"3D graphics visualizing parameters of a LLM model at each stage from\ntokenization to the output"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://github.com/mlabonne/llm-course",children:"LLM Course | GitHub @mlabonne"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Resources from mathematics, to Python, to neural networks, to NLP"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://spreadsheets-are-all-you-need.ai/index.html#watch-the-lessons",children:"Spreadsheets are all you need"}),"\n(",(0,s.jsx)(e.a,{href:"https://news.ycombinator.com/item?id=39700256",children:"HN"}),")","\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Understand GPT with Excel Spreadsheet"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"links",children:"Links"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://www.atmosera.com/ai/understanding-chatgpt/",children:"Understanding ChatGPT | Atmosera"}),"\n(",(0,s.jsx)(e.a,{href:"https://news.ycombinator.com/item?id=35312468",children:"HN"}),")","\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Understand how things went from RRN to LSTM to Transformer, to BERT, to GPT"}),"\n",(0,s.jsx)(e.li,{children:"Contains a brief explanation of each advancement and links to all the\nimportant papers"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://github.com/mlc-ai/web-llm",children:"Web LLM | GitHub @mlc-ai"})," \u2014 Running LLM\ndirectly in the browser"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://blog.replit.com/llm-training",children:"How Replit train their own Large Language Models"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Data processing (Databricks) \u2192 Custom tokenization \u2192 Model training\n(MosaicML) \u2192 Evaluation (HumanEval framework)"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://www.honeycomb.io/blog/hard-stuff-nobody-talks-about-llm",children:"All the Hard Stuff Nobody Talks About when Building Products with LLMs | Honeycomb.io"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"It's hard to build a real product backed by an LLM"}),"\n",(0,s.jsx)(e.li,{children:"Limited context windows, LLMs are slow and chaining is impractical, prompt\nengineering is weird, prompt injection, etc"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://simonwillison.net/2023/Jun/8/gpt-tokenizers/",children:"Understanding GPT tokenizers | Simon Willison"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Optimizations by including leading space in the token"}),"\n",(0,s.jsx)(e.li,{children:"The tokenization is biased towards English words"}),"\n",(0,s.jsx)(e.li,{children:"Glitch tokens: words that have no meaning but got tokenized, and get near 0\nweight after training lead to weird glitch"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better",children:"The history of open-source LLMs | Deep (Learning) Focus"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Nice graphs and tables visualizing the performances of different LLMs"}),"\n",(0,s.jsx)(e.li,{children:"Explains the evolution from lower-quality LLMs (BLOOM and OPT) to recent\npowerful models (LLaMA and MPT)"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://huyenchip.com/2023/08/16/llm-research-open-challenges.html",children:"10 open challenges in LLM research | Chip Huyen"}),"\n(",(0,s.jsx)(e.a,{href:"https://news.ycombinator.com/item?id=37155080",children:"HN"}),")","\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Reduce & measure hallucinations, optimize context construction, multimodal\ninputs, faster & cheaper, new architecture, GPU alternatives, agents acting\non behalf of LLM, human preference, chat interface, non-English language"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://benchmarks.llmonitor.com/",children:"Asking 60+ LLMs a set of 20 questions"}),"\n(",(0,s.jsx)(e.a,{href:"https://news.ycombinator.com/item?id=37445401",children:"HN"}),")","\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Benchmarking LLMs with some reflexion, knowledge, code, instructions and\ncreativity questions"}),"\n",(0,s.jsx)(e.li,{children:"More \"realistic\" benchmarks then those exams because it's likely it's\noutside the training set"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://ig.ft.com/generative-ai/",children:"How transformers work"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Nice graphics explaining concepts like embeddings, self-attention mechanism,\nbeam search and hallucination"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://www.anthropic.com/index/decomposing-language-models-into-understandable-components",children:"Decomposing Language Models Into Understandable Components"}),"\n(",(0,s.jsx)(e.a,{href:"https://news.ycombinator.com/item?id=37806861",children:"HN"}),")","\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'A single neuron does not have consistent meaning, but a group of neurons\ndoes, called "features"'}),"\n",(0,s.jsx)(e.li,{children:"Artificially activating features can steer the output of models, improving\nsecurity and our understanding of LLMs"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://github.com/spdustin/ChatGPT-AutoExpert/blob/main/System%20Prompts.md",children:"ChatGPT system prompts"}),"\n(",(0,s.jsx)(e.a,{href:"https://news.ycombinator.com/item?id=37879077",children:"HN"}),")","\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"https://old.reddit.com/r/OpenAI/comments/176mxj8/chatgpt_with_vision_system_prompt/k4r5lyh/",children:"How it's done by OP"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness",children:"Training great LLMs entirely from ground zero in the wilderness as a startup"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'Technology used, difficulties for startup "in the wild" (i.e. outside\nGoogle)'}),"\n",(0,s.jsx)(e.li,{children:"Some comparison of training in startup v.s. training with Google infra"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.a,{href:"https://www.lesswrong.com/posts/k38sJNLk7YbJA72ST/llm-generality-is-a-timeline-crux",children:"LLM Generality is a Timeline Crux | LessWrong"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Limitation exists, scaling, scaffolding and tooling can't fully overcome"}),"\n"]}),"\n"]}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,t.a)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(h,{...n})}):h(n)}},1151:(n,e,i)=>{i.d(e,{Z:()=>o,a:()=>l});var s=i(7294);const t={},r=s.createContext(t);function l(n){const e=s.useContext(r);return s.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:l(n.components),s.createElement(r.Provider,{value:e},n.children)}}}]);